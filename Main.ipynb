{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Link: https://archive.ics.uci.edu/dataset/462/drug+review+dataset+drugs+com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # for data preprocessing\n",
    "import itertools # for confusion matrix\n",
    "import string\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import joblib\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# If you want to show all the rows of pandas dataframe\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  drugName                     condition  \\\n",
       "0      206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260                Guanfacine                          ADHD   \n",
       "2       92703                    Lybrel                 Birth Control   \n",
       "3      138000                Ortho Evra                 Birth Control   \n",
       "4       35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "3  \"This is my first time using any form of birth...     8.0   \n",
       "4  \"Suboxone has completely turned my life around...     9.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  \n",
       "3   November 3, 2015           10  \n",
       "4  November 27, 2016           37  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('./drug review dataset drugs.com/drugsComTrain_raw.tsv',sep='\\t')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Information\n",
    "1. drugName (categorical): name of drug\n",
    "2. condition (categorical): name of condition\n",
    "3. review (text): patient review\n",
    "4. rating (numerical): 10 star patient rating\n",
    "5. date (date): date of review entry\n",
    "6. usefulCount (numerical): number of users who found review useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163740</td>\n",
       "      <td>Mirtazapine</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I&amp;#039;ve tried a few antidepressants over th...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>February 28, 2012</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206473</td>\n",
       "      <td>Mesalamine</td>\n",
       "      <td>Crohn's Disease, Maintenance</td>\n",
       "      <td>\"My son has Crohn&amp;#039;s disease and has done ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>May 17, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159672</td>\n",
       "      <td>Bactrim</td>\n",
       "      <td>Urinary Tract Infection</td>\n",
       "      <td>\"Quick reduction of symptoms\"</td>\n",
       "      <td>9.0</td>\n",
       "      <td>September 29, 2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39293</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"Contrave combines drugs that were used for al...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>March 5, 2017</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97768</td>\n",
       "      <td>Cyclafem 1 / 35</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I have been on this birth control for one cyc...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         drugName                     condition  \\\n",
       "0      163740      Mirtazapine                    Depression   \n",
       "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
       "2      159672          Bactrim       Urinary Tract Infection   \n",
       "3       39293         Contrave                   Weight Loss   \n",
       "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"I&#039;ve tried a few antidepressants over th...    10.0   \n",
       "1  \"My son has Crohn&#039;s disease and has done ...     8.0   \n",
       "2                      \"Quick reduction of symptoms\"     9.0   \n",
       "3  \"Contrave combines drugs that were used for al...     9.0   \n",
       "4  \"I have been on this birth control for one cyc...     9.0   \n",
       "\n",
       "                 date  usefulCount  \n",
       "0   February 28, 2012           22  \n",
       "1        May 17, 2009           17  \n",
       "2  September 29, 2017            3  \n",
       "3       March 5, 2017           35  \n",
       "4    October 22, 2015            4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('./drug review dataset drugs.com/drugsComTest_raw.tsv',sep='\\t')\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info(), data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape,data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1,data2],axis=0)\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=shuffle(data,random_state=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./drug review dataset drugs.com/DrugsComPatient_raw.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.read_csv('./drug review dataset drugs.com/DrugsComPatient_raw.csv')\n",
    "main_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = main_data[['condition','review']]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['condition'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['condition'].value_counts()>=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_counts = x[\"condition\"].value_counts()\n",
    "condition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_counts.head(5).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_condition = condition_counts[condition_counts>=4000].index\n",
    "len(valid_condition),valid_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[x['condition'].isin(valid_condition)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_5_ind=x['condition'].value_counts()[:5].index\n",
    "cond_5_val=x['condition'].value_counts()[:5].values\n",
    "\n",
    "sns.set_style(style=\"ticks\")\n",
    "fig= plt.figure(figsize=(5, 5))\n",
    "plt.pie(cond_5_val, labels=cond_5_ind, autopct='%.2f%%', startangle=120, colors=sns.color_palette(\"deep6\"))\n",
    "plt.title('Proportion Top 5 Conditions', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating dataframe for analyzing individual condition\n",
    "from wordcloud import WordCloud\n",
    "import PIL\n",
    "from IPython.display import Image\n",
    "img_mask = PIL.Image.open('./Med1.jpg')\n",
    "\n",
    "img_mask = np.array(img_mask)\n",
    "# display(img_mask)\n",
    "for condition in valid_condition:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    wc = WordCloud(max_words=200, \n",
    "                   colormap = 'BuPu_r', mask=img_mask, background_color='black').generate(' '.join(x[x['condition']==condition]['review']))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wc)\n",
    "    plt.title(f\"Word Cloud for {condition}\",fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['review'].str.contains('#')].reset_index(drop=True).loc[0,\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '#' in x[x['review'].str.contains('#')].reset_index(drop=True).loc[0,\"review\"]:\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(x.columns):\n",
    "    x.loc[:,col] = x.loc[:,col].str.replace('\"','')\n",
    "x[x['review'].str.contains('#')].reset_index(drop=True).loc[0,\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set the width of the column to maximum\n",
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are stopwords?\n",
    "\n",
    "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\n",
    "\n",
    "For example, in the English language, words like 'a', 'the', 'is', 'an', 'in', 'on', 'at', 'to', 'of', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "len(stopwords),stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "img=Image(filename='./stem_vs_lemma.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith(('V','N','R')):\n",
    "        return tag.lower()[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatize_word(words):\n",
    "    pos = pos_tag(words)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos:\n",
    "        pos = get_tag(tag)\n",
    "        if pos:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word,pos))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(raw_review):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,'html.parser').get_text()\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 3. Convert words to lower case and tokenize them\n",
    "    words = word_tokenize(review_text.lower())\n",
    "    # 4. Remove Punctuation\n",
    "    words = [word for word in words if word not in punct]\n",
    "    # 5. Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    # 6. Lemmatize words\n",
    "    words = lemmatize_word(words)\n",
    "    # 7. Join the words back into one string separated by space and return\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[:,'review_clean'] = x['review'].apply(clean_words)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_text = \"haha     jjjj    ksdkdk lsls\"\n",
    "    \n",
    "# words = review_text.lower().split()\n",
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat=x['review_clean']\n",
    "y=x['condition']\n",
    "x_feat.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "print(y, len(le.classes_),le.classes_,le.inverse_transform(y))\n",
    "joblib.dump(le,'./label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_feat,y,test_size=0.2,random_state=0,stratify=y)\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.mkdir(\"./Data\")\n",
    "joblib.dump(y_train, './Data/label_train.pkl')\n",
    "joblib.dump(y_test, './Data/label_test.pkl')\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words -> CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english',analyzer='word')\n",
    "len(count_vectorizer.get_stop_words()),count_vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(x_train)\n",
    "count_test = count_vectorizer.transform(x_test)\n",
    "\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.mkdir(\"./Data\")\n",
    "joblib.dump(count_train, './Data/count_train.pkl')\n",
    "joblib.dump(count_test, './Data/count_test.pkl')\n",
    "\n",
    "if not os.path.exists(\"./Bag of Words\"):\n",
    "    os.mkdir(\"./Bag of Words\")\n",
    "joblib.dump(count_vectorizer, \"./Bag of Words/count_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./Models\"):\n",
    "    os.mkdir(\"./Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=0.1) # alpha is the smoothing parameter, a constant that is added to the frequency of each word\n",
    "                               # to prevent zero probabilities\n",
    "mnb.fit(count_train,y_train)\n",
    "y_pred = mnb.predict(count_test)\n",
    "\n",
    "joblib.dump(mnb,'./Models/mnb_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix Naive Bayes\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15, weights='distance', leaf_size=30, p=2)\n",
    "knn.fit(count_train,y_train)\n",
    "y_pred = knn.predict(count_test)\n",
    "\n",
    "joblib.dump(knn, './Models/knn_w2v_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix K-Nearest Neighbors\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear', C=10)\n",
    "svc.fit(count_train,y_train)\n",
    "y_pred = svc.predict(count_test)\n",
    "\n",
    "joblib.dump(svc,'./Models/svc_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix Support Vector Machine - SVC\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = PassiveAggressiveClassifier(max_iter=1000, C=0.1, random_state=0)\n",
    "pac.fit(count_train,y_train)\n",
    "y_pred = pac.predict(count_test)\n",
    "\n",
    "joblib.dump(pac,'./Models/pac_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix Passive Aggresive Classifier\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000, C=10)\n",
    "lr.fit(count_train,y_train)\n",
    "y_pred = lr.predict(count_test)\n",
    "\n",
    "joblib.dump(lr,'./Models/lr_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix Logistic Regression\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50,random_state=0) # 50 n_estimators in order to fasten the process\n",
    "rf.fit(count_train,y_train)\n",
    "y_pred = rf.predict(count_test)\n",
    "\n",
    "joblib.dump(rf,'./Models/rf_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix Random Forest\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=1000,objective='multi:softmax')\n",
    "xgb.fit(count_train,y_train)\n",
    "y_pred = xgb.predict(count_test)\n",
    "\n",
    "joblib.dump(xgb,'./Models/xgb_countvec_model.pkl')\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "print(\"=====================================================\\n\")\n",
    "cm =confusion_matrix(y_test,y_pred,labels=range(len(le.classes_)))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix XGBoost\",fontsize=25)\n",
    "plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "plt.ylabel(\"True Value\",fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.get_params(),xgb.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words -> TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_gram=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer(tokenizer=word_tokenize,stop_words='english',token_pattern=None,ngram_range=(1,1))\n",
    "len(tfidf_vectorizer_unigram.get_stop_words()), tfidf_vectorizer_unigram.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_unigram = tfidf_vectorizer_unigram.fit_transform(x_train)\n",
    "tfidf_test_unigram = tfidf_vectorizer_unigram.transform(x_test)\n",
    "\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.mkdir(\"./Data\")\n",
    "joblib.dump(tfidf_train_unigram, './Data/tfidf_train_unigram.pkl')\n",
    "joblib.dump(tfidf_test_unigram, './Data/tfidf_test_unigram.pkl')\n",
    "\n",
    "if not os.path.exists(\"./Bag of Words\"):\n",
    "    os.mkdir(\"./Bag of Words\")\n",
    "joblib.dump(tfidf_vectorizer_unigram,'./Bag of Words/tfidf_vectorizer_(1,1)-gram_unigram.pkl')\n",
    "List_gram.append((\"TFIDF (1,1)-gram_Unigram\",tfidf_train_unigram,tfidf_test_unigram,\"(1,1)-gram_unigram\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_train_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFVectorizer: (1,2)-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tfidf_vectorizer_ubigram = TfidfVectorizer(tokenizer=word_tokenize,stop_words='english',token_pattern=None, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_ubigram = tfidf_vectorizer_ubigram.fit_transform(x_train)\n",
    "tfidf_test_ubigram = tfidf_vectorizer_ubigram.transform(x_test)\n",
    "\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.mkdir(\"./Data\")\n",
    "joblib.dump(tfidf_train_ubigram, './Data/tfidf_train_ubigram.pkl')\n",
    "joblib.dump(tfidf_test_ubigram, './Data/tfidf_test_ubigram.pkl')\n",
    "\n",
    "if not os.path.exists(\"./Bag of Words\"):\n",
    "    os.mkdir(\"./Bag of Words\")\n",
    "joblib.dump(tfidf_vectorizer_ubigram,'./Bag of Words/tfidf_vectorizer_(1,2)-gram.pkl')\n",
    "List_gram.append((\"TFIDF (1,2)-Gram\",tfidf_train_ubigram,tfidf_test_ubigram,\"(1,2)-gram\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_train_ubigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFVectorizer: (1,3)-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# tfidf_vectorizer_ubitrigram = TfidfVectorizer(tokenizer=word_tokenize,stop_words='english',token_pattern=None, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_train_ubitrigram = tfidf_vectorizer_ubitrigram.fit_transform(x_train)\n",
    "# tfidf_test_ubitrigram = tfidf_vectorizer_ubitrigram.transform(x_test)\n",
    "\n",
    "# if not os.path.exists(\"./Bag of Words\"):\n",
    "#     os.mkdir(\"./Bag of Words\")\n",
    "# joblib.dump(tfidf_vectorizer_ubitrigram,'./Bag of Words/tfidf_vectorizer_(1,3)-gram.pkl')\n",
    "# List_gram.append((\"TFIDF (1,3)-Gram\",tfidf_train_ubitrigram,tfidf_test_ubitrigram,\"(1,3)-gram\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_train_ubitrigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    mnb = MultinomialNB(alpha=0.1)\n",
    "    mnb.fit(x_tr,y_train)\n",
    "    y_pred = mnb.predict(x_ts)\n",
    "\n",
    "    joblib.dump(mnb,f'./Models/mnb_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix Naive Bayes - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    mnb_model.append(mnb) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=15, weights='distance', leaf_size=30, p=2)\n",
    "    knn.fit(x_tr,y_train)\n",
    "    y_pred = knn.predict(x_ts)\n",
    "\n",
    "    joblib.dump(knn,f'./Models/knn_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix K-Nearest Neighbors - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    knn_model.append(knn) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    svc = SVC(kernel='linear', C=10)\n",
    "    svc.fit(x_tr,y_train)\n",
    "    y_pred = svc.predict(x_ts)\n",
    "\n",
    "    joblib.dump(svc,f'./Models/svc_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix Support Vector Machine - SVC - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    svc_model.append(svc) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    pac = PassiveAggressiveClassifier(max_iter=1000, C=0.1, random_state=0)\n",
    "    pac.fit(x_tr,y_train)\n",
    "    y_pred = pac.predict(x_ts)\n",
    "\n",
    "    joblib.dump(pac,f'./Models/pac_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix Passive Aggresive Classifier - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    pac_model.append(pac) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    lr = LogisticRegression(max_iter=1000, C=10)\n",
    "    lr.fit(x_tr,y_train)\n",
    "    y_pred = lr.predict(x_ts)\n",
    "\n",
    "    joblib.dump(lr,f'./Models/lr_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix Logistic Regression - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    lr_model.append(lr) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    rf = RandomForestClassifier(n_estimators=50,random_state=0) # 50 n_estimators in order to fasten the process\n",
    "    rf.fit(x_tr,y_train)\n",
    "    y_pred = rf.predict(x_ts)\n",
    "\n",
    "    joblib.dump(rf,f'./Models/rf_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix Random Forest - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    rf_model.append(rf) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=[]\n",
    "for name, x_tr, x_ts, modname in List_gram:\n",
    "    print(f\"{name}\\n\")\n",
    "    xgb = xgboost.XGBClassifier(n_estimators=1000,objective='multi:softmax')\n",
    "    xgb.fit(x_tr,y_train)\n",
    "    y_pred = xgb.predict(x_ts)\n",
    "\n",
    "    joblib.dump(xgb,f'./Models/xgb_tfidf_{modname}_model.pkl')\n",
    "\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\\n\")\n",
    "    \n",
    "    print(f\"Classification report:\\n{classification_report(y_test,y_pred,digits=3)}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "    \n",
    "    cm =confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm,annot=True,fmt='d',cmap='coolwarm',xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "    plt.title(f\"Confusion Matrix XGBoost - {name}\",fontsize=25)\n",
    "    plt.xlabel(\"Predicted Value\",fontsize=20)\n",
    "    plt.ylabel(\"True Value\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    xgb_model.append(xgb) # 0 : Unigram (1,1), 1 : Unigram-Bigram (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n",
    "#     labelid = list(classifier.classes_).index(classlabel)\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n",
    "\n",
    "#     for coef, feat in topn:\n",
    "#         print(classlabel, feat, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(le.transform([\"Acne\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_ubigram.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform([\"Birth Control\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[100,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "main_data = pd.read_csv('./drug review dataset drugs.com/DrugsComPatient_raw.csv')\n",
    "main_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith(('V','N','R')):\n",
    "        return tag.lower()[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatize_word(words):\n",
    "    pos = pos_tag(words)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos:\n",
    "        pos = get_tag(tag)\n",
    "        if pos:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word,pos))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(raw_review):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,'html.parser').get_text()\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 3. Convert words to lower case and tokenize them\n",
    "    words = word_tokenize(review_text.lower())\n",
    "    # 4. Remove Punctuation\n",
    "    words = [word for word in words if word not in punct]\n",
    "    # 5. Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    # 6. Lemmatize words\n",
    "    words = lemmatize_word(words)\n",
    "    # 7. Join the words back into one string separated by space and return\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_drugs(label):\n",
    "    data_top = main_data[(main_data['rating']>=9) & (main_data['usefulCount']>=100)].sort_values(by=['rating','usefulCount'],ascending=[False, False])\n",
    "    data_top.head()\n",
    "    drug_list = data_top[data_top['condition']==label]['drugName'][:3].tolist()\n",
    "    return drug_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load LabelEncoder\n",
    "le = joblib.load('./label_encoder.pkl')\n",
    "\n",
    "# Load TFIDFVectorizer\n",
    "tfidf_ubigram = joblib.load('./Bag of Words/tfidf_vectorizer_(1,2)-gram.pkl')\n",
    "\n",
    "# Load Model\n",
    "models = {\n",
    "    \"Naive Bayes\": joblib.load(\"./Models/mnb_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"K-Nearest Neighbors\": joblib.load(\"./Models/knn_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"Support Vector Machine\": joblib.load(\"./Models/svc_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"Passive Aggressive Classifier\": joblib.load(\"./Models/pac_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"Logistic Regression\": joblib.load(\"./Models/lr_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"Random Forest\": joblib.load(\"./Models/rf_tfidf_(1,2)-gram_model.pkl\"),\n",
    "    \"XGBoost\": joblib.load(\"./Models/xgb_tfidf_(1,2)-gram_model.pkl\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Logistic Regression'].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Support Vector Machine'].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['XGBoost']\n",
    "def predict_condition(text):\n",
    "    text = [clean_words(text)]\n",
    "    text = tfidf_ubigram.transform(text)\n",
    "    pred = model.predict(text)[0]\n",
    "    return le.inverse_transform([pred])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I have situational depression, never dealt with it before. Wellbutrin had definitely helped get me out of bed each day. The &quot;situation&quot; is becoming better as well but I find my emotions aren&#039;t as out of whack. I cry less &amp; get angry less. I do feel it made me more anxious which I&#039;m already dealing with &amp; taking Buspirone for. I didn&#039;t experience many side effects BUT one that is hard to deal with but because it seems to make me get through each day a little better, I deal with it. It makes my throat feel closed, which is very annoying. I feel often like I can&#039;t breathe well &amp; that I can&#039;t swallow or that something is stuck in my throat. I&#039;ve not taken any other pills on several days to determine that it was the Wellbutrin that did it &amp; it is.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_condition(sentences[0])\n",
    "\n",
    "top_drugs = extract_top_drugs(prediction)\n",
    "\n",
    "print(\"Condition:\",prediction)\n",
    "print(\"Top Drugs:\")\n",
    "for i, drug in enumerate(top_drugs):\n",
    "    print(f\"{i+1}. {drug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Support Vector Machine'].coef_[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Logistic Regression'].coef_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Support Vector Machine'].coef_[0].toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Naive Bayes'].feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_top_features(vectorizer, classifier, class_label, le, top_features=10):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    class_index = le.transform([class_label])[0]\n",
    "    \n",
    "    if isinstance(classifier, (LogisticRegression, PassiveAggressiveClassifier)):\n",
    "        coef = classifier.coef_[class_index]\n",
    "    elif isinstance(classifier, MultinomialNB):\n",
    "        coef = classifier.feature_log_prob_[class_index]\n",
    "    elif isinstance(classifier, (RandomForestClassifier, xgboost.XGBClassifier, KNeighborsClassifier)):\n",
    "        print(f\"The model '{type(classifier).__name__}' does not provide direct feature importances.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"The model '{type(classifier).__name__}' has no suitable attribute for extracting feature importances. Unable to plot the top feature for label {class_label}.\")\n",
    "        return\n",
    "    \n",
    "    # if len(coef.shape) > 1:\n",
    "    #     coef = coef[class_index]\n",
    "\n",
    "    top_positive_coefficients = sorted(zip(coef, feature_names), key=lambda x: x[0], reverse=True)[:top_features]\n",
    "    top_negative_coefficients = sorted(zip(coef, feature_names), key=lambda x: x[0])[:top_features]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_coefficients = [coef for coef, feat in top_positive_coefficients]\n",
    "    top_features_names = [feat for coef, feat in top_positive_coefficients]\n",
    "    sns.barplot(x=top_coefficients, y=top_features_names, palette=\"Blues_d\",hue=top_features_names)\n",
    "    plt.title(f\"Top {top_features} Positive Features ({class_label})\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_coefficients = [coef for coef, feat in top_negative_coefficients]\n",
    "    top_features_names = [feat for coef, feat in top_negative_coefficients]\n",
    "    sns.barplot(x=top_coefficients, y=top_features_names, palette=\"Reds_d\",hue=top_features_names)\n",
    "    plt.title(f\"Top {top_features} Negative Features ({class_label})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "vectorizer = tfidf_ubigram\n",
    "classifier = models['Support Vector Machine']\n",
    "class_label = 'Depression'\n",
    "plot_top_features(vectorizer, classifier, class_label,le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
